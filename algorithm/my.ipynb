{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# Replace the file path with your dataset's file path\n",
    "df = pd.read_csv('updated_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "Order Date               0\n",
      "Product_Id               0\n",
      "Unnamed: 2           20000\n",
      "Quantity                 0\n",
      "Visibility               0\n",
      "Cart Count               0\n",
      "Cumulative Sales         0\n",
      "7-Day Moving Avg         0\n",
      "Sales Last 7 Days      100\n",
      "Trend                  100\n",
      "Sort Rank                0\n",
      "Interaction Score        0\n",
      "Stars                    0\n",
      "Reviews Count            0\n",
      "Base_Price               0\n",
      "Market_Price             0\n",
      "dtype: int64\n",
      "Missing values after filling:\n",
      "Order Date               0\n",
      "Product_Id               0\n",
      "Unnamed: 2           20000\n",
      "Quantity                 0\n",
      "Visibility               0\n",
      "Cart Count               0\n",
      "Cumulative Sales         0\n",
      "7-Day Moving Avg         0\n",
      "Sales Last 7 Days        0\n",
      "Trend                    0\n",
      "Sort Rank                0\n",
      "Interaction Score        0\n",
      "Stars                    0\n",
      "Reviews Count            0\n",
      "Base_Price               0\n",
      "Market_Price             0\n",
      "dtype: int64\n",
      "   Product_Id  Unnamed: 2  Quantity  Visibility  Cart Count  Cumulative Sales  \\\n",
      "0           0         NaN  1.636124    0.058345    1.636124         -1.694940   \n",
      "1           0         NaN  1.462993   -0.010484    1.462993         -1.662493   \n",
      "2           0         NaN -0.441450   -0.767602   -0.441450         -1.649874   \n",
      "3           0         NaN  0.251075   -0.492286    0.251075         -1.630045   \n",
      "4           0         NaN -1.653368   -1.249404   -1.653368         -1.630045   \n",
      "\n",
      "   7-Day Moving Avg  Sales Last 7 Days     Trend  Sort Rank  \\\n",
      "0         19.000000       8.586212e-16  0.000000   1.021989   \n",
      "1         18.500000      -2.830915e+00  2.436365   1.021989   \n",
      "2         14.666667      -1.743354e+00  0.252997   1.021989   \n",
      "3         13.750000      -1.320413e+00  0.735636   1.021989   \n",
      "4         11.000000      -6.557919e-01 -1.286852   1.021989   \n",
      "\n",
      "   Interaction Score     Stars  Reviews Count  Base_Price  Market_Price  Year  \\\n",
      "0           1.818251  0.336648       0.414430   -1.056419     -0.880527  2023   \n",
      "1          -0.405479  1.261794      -1.390454   -1.056419     -0.880527  2023   \n",
      "2          -0.596463 -1.538584      -0.535325   -1.056419     -1.065532  2023   \n",
      "3          -1.287203 -1.104872      -0.770135   -1.056419     -0.973029  2023   \n",
      "4          -0.924621 -1.709233       1.262550   -1.056419     -1.158034  2023   \n",
      "\n",
      "   Month  Day  Weekday  \n",
      "0      1    1        6  \n",
      "1      1    2        0  \n",
      "2      1    3        1  \n",
      "3      1    4        2  \n",
      "4      1    5        3  \n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"Missing values:\\n{missing_values}\")\n",
    "\n",
    "# Handle missing values (fill with the mean for numerical columns or mode for categorical columns)\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns  # Select only numeric columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())  # Fill numerical columns with mean\n",
    "\n",
    "# Handle categorical columns (fill with the mode)\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Check for missing values after filling\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"Missing values after filling:\\n{missing_values}\")\n",
    "\n",
    "# Convert 'Order Date' to datetime format\n",
    "if 'Order Date' in df.columns:  # Check if 'Order Date' exists before converting\n",
    "    df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
    "\n",
    "    # Feature engineering: Create new columns from the 'Order Date'\n",
    "    df['Year'] = df['Order Date'].dt.year\n",
    "    df['Month'] = df['Order Date'].dt.month\n",
    "    df['Day'] = df['Order Date'].dt.day\n",
    "    df['Weekday'] = df['Order Date'].dt.weekday  # Monday = 0, Sunday = 6\n",
    "\n",
    "    # Drop the original 'Order Date' column if not needed\n",
    "    df.drop(columns=['Order Date'], inplace=True)\n",
    "else:\n",
    "    print(\"The 'Order Date' column is missing in the dataset.\")\n",
    "\n",
    "# Encode categorical columns (e.g., if 'Product_Id' or others are categorical)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Normalize numerical features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Quantity', 'Visibility', 'Cart Count', 'Cumulative Sales', \n",
    "                  'Sales Last 7 Days', 'Trend', 'Sort Rank', 'Interaction Score', \n",
    "                  'Stars', 'Reviews Count', 'Base_Price', 'Market_Price']\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Check the processed dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Product_Id', 'Unnamed: 2', 'Quantity', 'Visibility', 'Cart Count',\n",
      "       'Cumulative Sales', '7-Day Moving Avg', 'Sales Last 7 Days', 'Trend',\n",
      "       'Sort Rank', 'Interaction Score', 'Stars', 'Reviews Count',\n",
      "       'Base_Price', 'Market_Price', 'Year', 'Month', 'Day', 'Weekday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.6533675481262433\n",
      "\n",
      "ElasticNet Evaluation:\n",
      "MSE: 0.0012473441348424515, R2: 0.9843321854649774\n",
      "\n",
      "Random Forest Evaluation:\n",
      "MSE: 9.756196693795924e-30, R2: 1.0\n",
      "\n",
      "XGBoost Evaluation:\n",
      "MSE: 0.010693782175288323, R2: 0.8656760463129844\n",
      "Epoch 1/50\n",
      "252/252 [==============================] - 1s 1ms/step - loss: 0.0837\n",
      "Epoch 2/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 0.0069\n",
      "Epoch 3/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 0.0029\n",
      "Epoch 4/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 0.0013\n",
      "Epoch 5/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 5.9516e-04\n",
      "Epoch 6/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 3.1812e-04\n",
      "Epoch 7/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 2.0913e-04\n",
      "Epoch 8/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.5519e-04\n",
      "Epoch 9/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.2392e-04\n",
      "Epoch 10/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.0438e-04\n",
      "Epoch 11/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 9.1430e-05\n",
      "Epoch 12/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 8.1265e-05\n",
      "Epoch 13/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 7.0702e-05\n",
      "Epoch 14/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 6.3947e-05\n",
      "Epoch 15/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 5.9700e-05\n",
      "Epoch 16/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 5.2390e-05\n",
      "Epoch 17/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 4.8242e-05\n",
      "Epoch 18/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 4.3593e-05\n",
      "Epoch 19/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 4.1802e-05\n",
      "Epoch 20/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 3.6672e-05\n",
      "Epoch 21/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 3.5148e-05\n",
      "Epoch 22/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 3.1585e-05\n",
      "Epoch 23/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 3.0244e-05\n",
      "Epoch 24/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 2.7720e-05\n",
      "Epoch 25/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 2.7766e-05\n",
      "Epoch 26/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 2.5621e-05\n",
      "Epoch 27/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 2.3163e-05\n",
      "Epoch 28/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.9826e-05\n",
      "Epoch 29/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.8126e-05\n",
      "Epoch 30/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.8373e-05\n",
      "Epoch 31/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.5864e-05\n",
      "Epoch 32/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.5332e-05\n",
      "Epoch 33/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.4678e-05\n",
      "Epoch 34/50\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 1.3639e-05\n",
      "Epoch 35/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.4531e-05\n",
      "Epoch 36/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.0319e-05\n",
      "Epoch 37/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.2253e-05\n",
      "Epoch 38/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.0638e-05\n",
      "Epoch 39/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.2428e-05\n",
      "Epoch 40/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 9.5197e-06\n",
      "Epoch 41/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 9.1527e-06\n",
      "Epoch 42/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.1016e-05\n",
      "Epoch 43/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 9.9466e-06\n",
      "Epoch 44/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 6.1562e-06\n",
      "Epoch 45/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 1.0150e-05\n",
      "Epoch 46/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 8.4824e-06\n",
      "Epoch 47/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 6.4137e-06\n",
      "Epoch 48/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 6.3239e-06\n",
      "Epoch 49/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 5.8584e-06\n",
      "Epoch 50/50\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 8.6285e-06\n",
      "63/63 [==============================] - 0s 738us/step\n",
      "\n",
      "LSTM Evaluation:\n",
      "MSE: 8.28242541969597e-06, R2: 0.9998959649532546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supran\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Supran\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Supran\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Supran\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Supran\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Supran\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ARIMA Evaluation:\n",
      "MSE: 0.9642571358069402, R2: -11.111975793916288\n",
      "\n",
      "Ensemble Evaluation:\n",
      "MSE: 0.00021742405539251078, R2: 0.9972689495383144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supran\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\Supran\\AppData\\Roaming\\Python\\Python311\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and is already loaded\n",
    "# Check for non-positive values in Quantity and replace them or filter out\n",
    "print(df['Quantity'].min())  # Check if there's any zero or negative value\n",
    "\n",
    "# Drop rows with non-positive Quantity or apply a small value adjustment\n",
    "df = df[df['Quantity'] > 0]\n",
    "\n",
    "# Feature Selection\n",
    "X = df[['Visibility', 'Cart Count', 'Cumulative Sales', 'Sales Last 7 Days',\n",
    "        'Trend', 'Sort Rank', 'Interaction Score', 'Stars',\n",
    "        'Reviews Count', 'Base_Price', 'Market_Price']]\n",
    "Y = df['Quantity']\n",
    "\n",
    "# Apply log1p transformation to Quantity\n",
    "Y = np.log1p(Y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "### 1. Linear Regression with ElasticNet Regularization\n",
    "elastic_net = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=1000)\n",
    "elastic_net.fit(X_train, Y_train)\n",
    "Y_pred_en = elastic_net.predict(X_test)\n",
    "\n",
    "# Evaluate ElasticNet\n",
    "mse_en = mean_squared_error(Y_test, Y_pred_en)\n",
    "r2_en = r2_score(Y_test, Y_pred_en)\n",
    "print(\"\\nElasticNet Evaluation:\")\n",
    "print(f\"MSE: {mse_en}, R2: {r2_en}\")\n",
    "\n",
    "### 2. Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "mse_rf = mean_squared_error(Y_test, Y_pred_rf)\n",
    "r2_rf = r2_score(Y_test, Y_pred_rf)\n",
    "print(\"\\nRandom Forest Evaluation:\")\n",
    "print(f\"MSE: {mse_rf}, R2: {r2_rf}\")\n",
    "\n",
    "### 3. XGBoost Regressor\n",
    "xgb = XGBRegressor(learning_rate=0.01, n_estimators=100, max_depth=5, random_state=42)\n",
    "xgb.fit(X_train, Y_train)\n",
    "Y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "mse_xgb = mean_squared_error(Y_test, Y_pred_xgb)\n",
    "r2_xgb = r2_score(Y_test, Y_pred_xgb)\n",
    "print(\"\\nXGBoost Evaluation:\")\n",
    "print(f\"MSE: {mse_xgb}, R2: {r2_xgb}\")\n",
    "\n",
    "### 4. LSTM for Temporal Analysis\n",
    "# Reshape data for LSTM\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# LSTM Model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(1, X_train.shape[1])),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "lstm_model.fit(X_train_lstm, Y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Predict and Evaluate LSTM\n",
    "Y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "mse_lstm = mean_squared_error(Y_test, Y_pred_lstm)\n",
    "r2_lstm = r2_score(Y_test, Y_pred_lstm)\n",
    "print(\"\\nLSTM Evaluation:\")\n",
    "print(f\"MSE: {mse_lstm}, R2: {r2_lstm}\")\n",
    "\n",
    "### 5. ARIMA for Trend and Seasonality\n",
    "# Recreate 'Order Date' from Year, Month, and Day columns\n",
    "df['Order Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "\n",
    "# Set 'Order Date' as index\n",
    "df.set_index('Order Date', inplace=True)\n",
    "\n",
    "# Prepare quantity series\n",
    "quantity_series = df['Quantity']\n",
    "\n",
    "# ARIMA Model (p=1, d=2, q=1 as per ACF and PACF)\n",
    "arima_model = ARIMA(quantity_series, order=(1, 2, 1))\n",
    "arima_result = arima_model.fit()\n",
    "arima_forecast = arima_result.forecast(steps=len(Y_test))\n",
    "\n",
    "# Evaluate ARIMA\n",
    "mse_arima = mean_squared_error(Y_test[:len(arima_forecast)], arima_forecast)\n",
    "r2_arima = r2_score(Y_test[:len(arima_forecast)], arima_forecast)\n",
    "print(\"\\nARIMA Evaluation:\")\n",
    "print(f\"MSE: {mse_arima}, R2: {r2_arima}\")\n",
    "\n",
    "### Ensemble of Predictions\n",
    "# Median of all predictions\n",
    "Y_pred_ensemble = np.median(\n",
    "    np.vstack([Y_pred_en, Y_pred_rf, Y_pred_xgb, Y_pred_lstm.flatten()[:len(Y_pred_en)]]), axis=0)\n",
    "\n",
    "# Evaluate Ensemble\n",
    "mse_ensemble = mean_squared_error(Y_test, Y_pred_ensemble)\n",
    "r2_ensemble = r2_score(Y_test, Y_pred_ensemble)\n",
    "print(\"\\nEnsemble Evaluation:\")\n",
    "print(f\"MSE: {mse_ensemble}, R2: {r2_ensemble}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Quantity (Log Scale): 0.8276808261871338\n",
      "Predicted Quantity (Original Scale): 1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supran\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the model you want to use for predictions\n",
    "model = arima_model  # Replace with rf, xgb, lstm_model, or arima_model as needed\n",
    "\n",
    "# New data point for prediction\n",
    "new_data = np.array([[2888, 3, 200, 30, 0.8, 5, 10, 4.5, 25, 800, 1500]])  # Replace with your actual values\n",
    "\n",
    "# Scale the new data using the same scaler used for training\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# ElasticNet, RandomForest, or XGBoost Prediction\n",
    "if model in [elastic_net, rf, xgb]:\n",
    "    predicted_quantity = model.predict(new_data_scaled)\n",
    "    predicted_quantity_original = np.expm1(predicted_quantity)  # Inverse transform to original scale\n",
    "    print(f\"Predicted Quantity (Log Scale): {predicted_quantity[0]}\")\n",
    "    print(f\"Predicted Quantity (Original Scale): {predicted_quantity_original[0]:.2f}\")\n",
    "\n",
    "# LSTM Prediction\n",
    "elif model == lstm_model:\n",
    "    new_data_lstm = new_data_scaled.reshape((new_data_scaled.shape[0], 1, new_data_scaled.shape[1]))\n",
    "    predicted_quantity = model.predict(new_data_lstm)\n",
    "    predicted_quantity_original = np.expm1(predicted_quantity)  # Inverse transform to original scale\n",
    "    print(f\"Predicted Quantity (Log Scale): {predicted_quantity[0][0]}\")\n",
    "    print(f\"Predicted Quantity (Original Scale): {predicted_quantity_original[0][0]:.2f}\")\n",
    "\n",
    "# ARIMA Prediction\n",
    "elif model == arima_model:\n",
    "    # For ARIMA, provide a time series of features if required. Simplified here for one step.\n",
    "    arima_forecast = model.forecast(steps=1)\n",
    "    predicted_quantity = arima_forecast[0]\n",
    "    predicted_quantity_original = np.expm1(predicted_quantity)  # Inverse transform to original scale\n",
    "    print(f\"Predicted Quantity (Log Scale): {predicted_quantity}\")\n",
    "    print(f\"Predicted Quantity (Original Scale): {predicted_quantity_original:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
